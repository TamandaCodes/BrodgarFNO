{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hG7KQmwupOb9",
        "YQCv2iaN52q5"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TamandaCodes/BrodgarFNO/blob/main/Brodgar_Transient_FNO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sHcQjWN8twq",
        "outputId": "a3fa58f7-a72d-4740-feda-634fbe1b99a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive (/content/gdrive) already mounted\n"
          ]
        }
      ],
      "source": [
        "# MOUNT THE DRIVE!!!\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "\n",
        "drive_name = '/content/gdrive'\n",
        "if os.path.ismount(drive_name):\n",
        "    print(f'Drive ({drive_name}) already mounted')\n",
        "else:\n",
        "    drive.mount(drive_name)\n",
        "\n",
        "# Project dir (where artifacts & outputs will be written)\n",
        "dir_path = '/content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder'\n",
        "os.makedirs(dir_path, exist_ok=True)\n",
        "sys.path.insert(1, dir_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fno_full_block_train_infer.py\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.fft as fft\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from typing import Tuple, Dict, Any\n",
        "import numpy as np\n",
        "from data_store_load import load_full_dataset\n",
        "from fno_implementation_modified import (\n",
        "    compute_norm_stats, FNO2DSpacetime, OneStepDataset,\n",
        "    collate_batch, train_one_epoch, eval_one_epoch, autoregressive_rollout\n",
        ")"
      ],
      "metadata": {
        "id": "Un8ZaGAD9aX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- imports you rely on (make sure these exist up top) ---\n",
        "import os, math, torch, torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# =========================\n",
        "# Root dir & path helpers\n",
        "# =========================\n",
        "try:\n",
        "    DIR = dir_path\n",
        "except Exception:\n",
        "    DIR = \"\"\n",
        "\n",
        "def P(fn: str) -> str:\n",
        "    \"Join to project directory for outputs/artifacts.\"\n",
        "    return os.path.join(DIR, fn)\n",
        "\n",
        "def find_input(filename: str) -> str:\n",
        "    cands = [\n",
        "        os.path.join(DIR, filename),\n",
        "        os.path.join('/mnt/data', filename),\n",
        "        filename,  # absolute / custom\n",
        "    ]\n",
        "    for p in cands:\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    raise FileNotFoundError(f\"Could not find input file '{filename}' in {cands}\")\n",
        "\n",
        "# Checkpoint path\n",
        "ckpt_path = P(\"fno_autoregressive.pt\")"
      ],
      "metadata": {
        "id": "23oMv--_l3-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class H1Loss(nn.Module):\n",
        "    \"\"\"\n",
        "    Sobolev Loss (H1): Penalizes errors in value AND errors in spatial slope (derivative).\n",
        "    Use this to force the model to capture high-frequency 'wiggles'.\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.1):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha  # Weight for the derivative term\n",
        "        self.l1 = nn.L1Loss() # Base loss (L1 is sharper than MSE)\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # 1. Standard L1 Loss on values (Magnitude accuracy)\n",
        "        loss_val = self.l1(pred, target)\n",
        "\n",
        "        # 2. L1 Loss on Spatial Gradients (Shape/Wiggle accuracy)\n",
        "        # Calculate gradients along the spatial dimension (last dim, X)\n",
        "        # Input shape is usually [B, C, T, X] or [B, C, 1, X]\n",
        "        pred_dx = pred[..., 1:] - pred[..., :-1]\n",
        "        target_dx = target[..., 1:] - target[..., :-1]\n",
        "\n",
        "        loss_grad = self.l1(pred_dx, target_dx)\n",
        "\n",
        "        # Total Loss\n",
        "        return loss_val + (self.alpha * loss_grad)"
      ],
      "metadata": {
        "id": "ovINor2AAovV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAIN"
      ],
      "metadata": {
        "id": "6mY6FzfoDWdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "#           MAIN\n",
        "# =========================\n",
        "def main():\n",
        "    print(\"=\"*80)\n",
        "    print(\"FNO 1D SPACE PIPELINE TRANSIENT MODEL\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "   # ---- Configuration ----\n",
        "    IN_CHANNELS = 10   # All 10 variables as input\n",
        "    OUT_CHANNELS = 7   # First 7 variables as output\n",
        "\n",
        "    # ---- paths ----\n",
        "    dataset_file = find_input(\"cleaned_tensor.pt\")\n",
        "    print(f\"\\n1. Configuration\")\n",
        "    print(f\"   Dataset file: {dataset_file}\")\n",
        "    print(f\"   Input channels: {IN_CHANNELS}\")\n",
        "    print(f\"   Output channels: {OUT_CHANNELS}\")\n",
        "\n",
        "    # ---- check if dataset exists ----\n",
        "    if not os.path.exists(dataset_file):\n",
        "        print(f\"\\n‚ùå Error: Dataset file not found: {dataset_file}\")\n",
        "        print(\"   Please run data_store_load.py first to create the dataset.\")\n",
        "        return # Return None if dataset not found\n",
        "\n",
        "     # ---- load dataset ----\n",
        "    print(f\"\\n2. Loading dataset...\")\n",
        "    try:\n",
        "        # Load your tensor format\n",
        "        data = torch.load(dataset_file)\n",
        "\n",
        "        # Handle different possible formats\n",
        "        if isinstance(data, dict):\n",
        "            full_tensor = data['full_tensor']  # [50, 1279, 10, 288]\n",
        "            variable_names = data.get('variable_names', [f'Var{i}' for i in range(IN_CHANNELS)])\n",
        "        else:\n",
        "            full_tensor = data\n",
        "            variable_names = [f'Var{i}' for i in range(IN_CHANNELS)]\n",
        "\n",
        "        N, X, C, T = full_tensor.shape\n",
        "\n",
        "        print(f\"   ‚úì Dataset shape: {full_tensor.shape}\")\n",
        "        print(f\"   ‚úì Cases: {N}\")\n",
        "        print(f\"   ‚úì Sections: {X}\")\n",
        "        print(f\"   ‚úì Variables: {C}\")\n",
        "        print(f\"   ‚úì Timesteps: {T}\")\n",
        "        print(f\"   ‚úì Variable names: {variable_names}\")\n",
        "\n",
        "        # Validate dimensions\n",
        "        assert C >= IN_CHANNELS, f\"Expected at least {IN_CHANNELS} channels, got {C}\"\n",
        "        assert C >= OUT_CHANNELS, f\"Expected at least {OUT_CHANNELS} channels, got {C}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading dataset: {e}\")\n",
        "        return # Return None if error loading dataset\n",
        "\n",
        "    # Create section lengths (uniform or load actual if available)\n",
        "    section_lengths = np.arange(X, dtype=np.float32)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"   ‚úì Using device: {device}\")\n",
        "\n",
        "    # ---- splits (by case) ----\n",
        "    print(f\"\\n3. Creating train/val/test splits...\")\n",
        "    N = full_tensor.shape[0]\n",
        "    n_train = int(0.8 * N)\n",
        "    n_val   = int(0.1 * N)\n",
        "    n_test  = N - n_train - n_val\n",
        "\n",
        "    print(f\"   Total cases: {N}\")\n",
        "    print(f\"   Train: {n_train} cases ({100*n_train/N:.0f}%)\")\n",
        "    print(f\"   Val: {n_val} cases ({100*n_val/N:.0f}%)\")\n",
        "    print(f\"   Test: {n_test} cases ({100*n_test/N:.0f}%)\")\n",
        "\n",
        "    # simple contiguous split; switch to random if you prefer\n",
        "    train_tensor = full_tensor[:n_train]\n",
        "    val_tensor   = full_tensor[n_train:n_train+n_val]\n",
        "    test_tensor  = full_tensor[n_train+n_val:]\n",
        "\n",
        "     # ---- normalization on train only ----\n",
        "    print(f\"\\n4. Computing normalization statistics...\")\n",
        "    mean_input, std_input, mean_output, std_output = compute_norm_stats(\n",
        "        train_tensor, IN_CHANNELS, OUT_CHANNELS\n",
        "    )\n",
        "\n",
        "    print(\"   Input variables (10):\")\n",
        "    for i in range(IN_CHANNELS):\n",
        "        var_name = variable_names[i] if i < len(variable_names) else f'Var{i}'\n",
        "        print(f\"     {var_name}: mean={mean_input[i]:.3e}, std={std_input[i]:.3e}\")\n",
        "\n",
        "    print(\"   Output variables (7):\")\n",
        "    for i in range(OUT_CHANNELS):\n",
        "        var_name = variable_names[i] if i < len(variable_names) else f'Var{i}'\n",
        "        print(f\"     {var_name}: mean={mean_output[i]:.3e}, std={std_output[i]:.3e}\")\n",
        "\n",
        "\n",
        "    # ---- datasets & loaders ----\n",
        "    print(f\"\\n5. Creating datasets and dataloaders...\")\n",
        "\n",
        "    # IMPORTANT: your dataset already returns CUDA tensors when device=\"cuda\".\n",
        "    # To avoid the pin_memory crash, either:\n",
        "    #   A) keep device=cuda and DISABLE pin_memory, or\n",
        "    #   B) force dataset to return CPU tensors and move to device in the loop.\n",
        "    # We'll do (A) for minimal change.\n",
        "    train_ds = OneStepDataset(\n",
        "        train_tensor, section_lengths,\n",
        "        mean_input, std_input, mean_output, std_output,\n",
        "        IN_CHANNELS, OUT_CHANNELS, device=device\n",
        "    )\n",
        "    val_ds = OneStepDataset(\n",
        "        val_tensor, section_lengths,\n",
        "        mean_input, std_input, mean_output, std_output,\n",
        "        IN_CHANNELS, OUT_CHANNELS, device=device\n",
        "    )\n",
        "    test_ds = OneStepDataset(\n",
        "        test_tensor, section_lengths,\n",
        "        mean_input, std_input, mean_output, std_output,\n",
        "        IN_CHANNELS, OUT_CHANNELS, device=device\n",
        "    )\n",
        "\n",
        "    print(f\"   Train samples: {len(train_ds)} ({n_train} cases √ó {T-1} timesteps)\")\n",
        "    print(f\"   Val samples: {len(val_ds)}\")\n",
        "    print(f\"   Test samples: {len(test_ds)}\")\n",
        "\n",
        "    batch_size = 32 if device == \"cuda\" else 4\n",
        "    print(f\"   Batch size: {batch_size}\")\n",
        "\n",
        "    # If tensors are already on CUDA, pin_memory must be False (pinning works only for CPU tensors).\n",
        "    use_pin_memory = False\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=batch_size, shuffle=True, num_workers=0,\n",
        "        pin_memory=use_pin_memory, collate_fn=collate_batch\n",
        "    )\n",
        "    val_loader   = DataLoader(\n",
        "        val_ds,   batch_size=batch_size, shuffle=False, num_workers=0,\n",
        "        pin_memory=use_pin_memory, collate_fn=collate_batch\n",
        "    )\n",
        "\n",
        "    # ---- model ----\n",
        "   # in_ch = IN_CHANNELS + 3 (t_coord, x_coord, t0_mask)\n",
        "    model_in_ch = IN_CHANNELS + 3\n",
        "\n",
        "    model = FNO2DSpacetime(\n",
        "        in_ch=model_in_ch,\n",
        "        out_ch=OUT_CHANNELS,\n",
        "        width=256, #Changed from 128 -> 256\n",
        "        depth=5,\n",
        "        k_t=1,   # KEY: Single mode in time (essentially spatial-only)\n",
        "        k_x=128,  # Spatial modes; Changed from 64 -> 128\n",
        "        predict_delta_from_u0=False\n",
        "    ).to(device)\n",
        "\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"   ‚úì Model created\")\n",
        "    print(f\"   ‚úì Parameters: {n_params:,}\")\n",
        "    print(f\"   ‚úì Input channels: {model_in_ch} ({IN_CHANNELS} vars + 3 coords)\")\n",
        "    print(f\"   ‚úì Output channels: {OUT_CHANNELS}\")\n",
        "    print(f\"   ‚úì k_t: 1 (spatial-only)\")\n",
        "    print(f\"   ‚úì k_x: 96\")\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    loss_fn = H1Loss(alpha=0.5)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    # Fix deprecated GradScaler usage\n",
        "    scaler = torch.amp.GradScaler('cuda') if device == \"cuda\" else None\n",
        "\n",
        "    # ---- training loop with early stopping ----\n",
        "    max_epochs = 100\n",
        "    early_stopping_patience = 10\n",
        "\n",
        "    print(f\"\\n7. Starting training...\")\n",
        "    print(f\"   Max epochs: {max_epochs}\")\n",
        "    print(f\"   Early stopping patience: {early_stopping_patience}\")\n",
        "    print(f\"   Learning rate: 1e-3\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    best_val = math.inf\n",
        "    bad = 0\n",
        "\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        epoch_start = time.time()\n",
        "        print(f\"\\n[Epoch {epoch:03d}/{max_epochs}]\")\n",
        "\n",
        "        tr = train_one_epoch(model, train_loader, opt, loss_fn, scaler=scaler)\n",
        "        va = eval_one_epoch(model, val_loader, loss_fn)\n",
        "\n",
        "        train_losses.append(tr)\n",
        "        val_losses.append(va)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        current_lr = opt.param_groups[0]['lr']\n",
        "\n",
        "        sched.step(va)\n",
        "\n",
        "        improvement = \"\"\n",
        "        if va + 1e-8 < best_val:\n",
        "            improvement = f\" ‚≠ê NEW BEST! (improved by {(best_val - va):.6f})\"\n",
        "\n",
        "        print(f\"  Results: Train Loss = {tr:.6f} | Val Loss = {va:.6f}\")\n",
        "        print(f\"  Time: {epoch_time:.1f}s | LR: {current_lr:.2e}{improvement}\")\n",
        "\n",
        "        if va + 1e-8 < best_val:\n",
        "            best_val = va\n",
        "            bad = 0\n",
        "            # Save checkpoint\n",
        "            os.makedirs(os.path.dirname(ckpt_path) or \".\", exist_ok=True)\n",
        "            torch.save({\n",
        "                \"model\": model.state_dict(),\n",
        "                \"mean_input\": mean_input,\n",
        "                \"std_input\": std_input,\n",
        "                \"mean_output\": mean_output,\n",
        "                \"std_output\": std_output,\n",
        "                \"variable_names\": variable_names,\n",
        "                \"section_lengths\": torch.tensor(section_lengths, dtype=torch.float32),\n",
        "                \"in_channels\": IN_CHANNELS,\n",
        "                \"out_channels\": OUT_CHANNELS,\n",
        "                \"config\": {\n",
        "                    \"width\": 256,\n",
        "                    \"depth\": 5,\n",
        "                    \"k_t\": 1,\n",
        "                    \"k_x\": 128,\n",
        "                }\n",
        "            }, ckpt_path)\n",
        "            print(f\"  ‚úì Checkpoint saved to: {ckpt_path}\")\n",
        "        else:\n",
        "            bad += 1\n",
        "            print(f\"  No improvement ({bad}/{early_stopping_patience})\")\n",
        "            if bad >= early_stopping_patience:\n",
        "                total_time = time.time() - start_time\n",
        "                print(f\"\\n  ‚ö† Early stopping triggered after {epoch} epochs ({total_time/60:.1f} minutes)\")\n",
        "                break\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n\" + \"=\" * 80)\n",
        "    print(f\"TRAINING COMPLETE!\")\n",
        "    print(f\"  Total time: {total_time/60:.1f} minutes\")\n",
        "    print(f\"  Best validation loss: {best_val:.6f}\")\n",
        "    print(f\"  Final epoch: {epoch}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # ---- inference demo on one test case ----\n",
        "    \"\"\"if len(test_tensor) > 0:\n",
        "        case0 = test_tensor[0]  # [X,5,121]\n",
        "        ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "        model.eval()\n",
        "        preds = predict_full_block(model, case0, section_lengths, mean, std, device=device)  # [5,120,X]\n",
        "        var_idx = variable_names.index(\"PT\") if \"PT\" in variable_names else 0\n",
        "        pt_pred = preds[var_idx]  # [120,X]\n",
        "        print(f\"Inference OK. PT prediction block: mean={pt_pred.mean().item():.4f}, std={pt_pred.std().item():.4f}\")\"\"\"\n",
        "\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    # ---- Plot training curves ----\n",
        "    print(f\"\\n8. Plotting training curves...\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.yscale('log')\n",
        "    curves_path = P('training_curves.png')\n",
        "    plt.savefig(curves_path, dpi=150)\n",
        "    print(f\"  ‚úì Saved: {curves_path}\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Call main\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOAJXjXB9_6w",
        "outputId": "de5b692a-b6a4-4771-fcfc-119e43891b5a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "FNO 1D SPACE PIPELINE TRANSIENT MODEL\n",
            "================================================================================\n",
            "\n",
            "1. Configuration\n",
            "   Dataset file: /content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder/cleaned_tensor.pt\n",
            "   Input channels: 10\n",
            "   Output channels: 7\n",
            "\n",
            "2. Loading dataset...\n",
            "   ‚úì Dataset shape: torch.Size([50, 1279, 10, 288])\n",
            "   ‚úì Cases: 50\n",
            "   ‚úì Sections: 1279\n",
            "   ‚úì Variables: 10\n",
            "   ‚úì Timesteps: 288\n",
            "   ‚úì Variable names: ['PT', 'TM', 'HOL', 'HOLWT', 'UG', 'UL', 'ULWT', 'TT', 'PI', 'FI']\n",
            "   ‚úì Using device: cuda\n",
            "\n",
            "3. Creating train/val/test splits...\n",
            "   Total cases: 50\n",
            "   Train: 40 cases (80%)\n",
            "   Val: 5 cases (10%)\n",
            "   Test: 5 cases (10%)\n",
            "\n",
            "4. Computing normalization statistics...\n",
            "   Input variables (10):\n",
            "     PT: mean=5.287e+01, std=1.649e+01\n",
            "     TM: mean=3.784e+01, std=1.281e+01\n",
            "     HOL: mean=1.255e-01, std=3.184e-02\n",
            "     HOLWT: mean=7.121e-02, std=3.003e-02\n",
            "     UG: mean=7.286e+00, std=2.392e+00\n",
            "     UL: mean=2.443e+00, std=9.425e-01\n",
            "     ULWT: mean=2.046e+00, std=9.085e-01\n",
            "     TT: mean=6.081e+01, std=5.457e+00\n",
            "     PI: mean=2.886e+01, std=5.668e-01\n",
            "     FI: mean=7.216e+01, std=2.824e+01\n",
            "   Output variables (7):\n",
            "     PT: mean=8.363e-04, std=1.938e-01\n",
            "     TM: mean=1.458e-03, std=7.752e-02\n",
            "     HOL: mean=-1.138e-06, std=1.339e-03\n",
            "     HOLWT: mean=-1.377e-06, std=1.280e-03\n",
            "     UG: mean=-1.476e-04, std=4.928e-02\n",
            "     UL: mean=-3.213e-06, std=3.973e-02\n",
            "     ULWT: mean=1.838e-05, std=3.855e-02\n",
            "\n",
            "5. Creating datasets and dataloaders...\n",
            "   Train samples: 11480 (40 cases √ó 287 timesteps)\n",
            "   Val samples: 1435\n",
            "   Test samples: 1435\n",
            "   Batch size: 32\n",
            "   ‚úì Model created\n",
            "   ‚úì Parameters: 84,286,215\n",
            "   ‚úì Input channels: 13 (10 vars + 3 coords)\n",
            "   ‚úì Output channels: 7\n",
            "   ‚úì k_t: 1 (spatial-only)\n",
            "   ‚úì k_x: 96\n",
            "\n",
            "7. Starting training...\n",
            "   Max epochs: 100\n",
            "   Early stopping patience: 10\n",
            "   Learning rate: 1e-3\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[Epoch 001/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.445095 | Val Loss = 0.336197\n",
            "  Time: 72.8s | LR: 1.00e-03 ‚≠ê NEW BEST! (improved by inf)\n",
            "  ‚úì Checkpoint saved to: /content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder/fno_autoregressive.pt\n",
            "\n",
            "[Epoch 002/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.372913 | Val Loss = 0.308762\n",
            "  Time: 74.2s | LR: 1.00e-03 ‚≠ê NEW BEST! (improved by 0.027435)\n",
            "  ‚úì Checkpoint saved to: /content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder/fno_autoregressive.pt\n",
            "\n",
            "[Epoch 003/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.353881 | Val Loss = 0.314046\n",
            "  Time: 74.4s | LR: 1.00e-03\n",
            "  No improvement (1/10)\n",
            "\n",
            "[Epoch 004/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.346101 | Val Loss = 0.294740\n",
            "  Time: 73.2s | LR: 1.00e-03 ‚≠ê NEW BEST! (improved by 0.014022)\n",
            "  ‚úì Checkpoint saved to: /content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder/fno_autoregressive.pt\n",
            "\n",
            "[Epoch 005/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.339682 | Val Loss = 0.294667\n",
            "  Time: 74.3s | LR: 1.00e-03 ‚≠ê NEW BEST! (improved by 0.000073)\n",
            "  ‚úì Checkpoint saved to: /content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder/fno_autoregressive.pt\n",
            "\n",
            "[Epoch 006/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.332546 | Val Loss = 0.295606\n",
            "  Time: 74.3s | LR: 1.00e-03\n",
            "  No improvement (1/10)\n",
            "\n",
            "[Epoch 007/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.325484 | Val Loss = 0.292108\n",
            "  Time: 73.4s | LR: 1.00e-03 ‚≠ê NEW BEST! (improved by 0.002559)\n",
            "  ‚úì Checkpoint saved to: /content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder/fno_autoregressive.pt\n",
            "\n",
            "[Epoch 008/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.320345 | Val Loss = 0.287257\n",
            "  Time: 74.2s | LR: 1.00e-03 ‚≠ê NEW BEST! (improved by 0.004851)\n",
            "  ‚úì Checkpoint saved to: /content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder/fno_autoregressive.pt\n",
            "\n",
            "[Epoch 009/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.315529 | Val Loss = 0.305920\n",
            "  Time: 74.1s | LR: 1.00e-03\n",
            "  No improvement (1/10)\n",
            "\n",
            "[Epoch 010/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.310203 | Val Loss = 0.292561\n",
            "  Time: 73.2s | LR: 1.00e-03\n",
            "  No improvement (2/10)\n",
            "\n",
            "[Epoch 011/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.305336 | Val Loss = 0.297358\n",
            "  Time: 73.4s | LR: 1.00e-03\n",
            "  No improvement (3/10)\n",
            "\n",
            "[Epoch 012/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.299694 | Val Loss = 0.292891\n",
            "  Time: 73.2s | LR: 1.00e-03\n",
            "  No improvement (4/10)\n",
            "\n",
            "[Epoch 013/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.294291 | Val Loss = 0.292365\n",
            "  Time: 73.3s | LR: 1.00e-03\n",
            "  No improvement (5/10)\n",
            "\n",
            "[Epoch 014/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.289530 | Val Loss = 0.313734\n",
            "  Time: 73.2s | LR: 1.00e-03\n",
            "  No improvement (6/10)\n",
            "\n",
            "[Epoch 015/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.266106 | Val Loss = 0.300408\n",
            "  Time: 73.4s | LR: 5.00e-04\n",
            "  No improvement (7/10)\n",
            "\n",
            "[Epoch 016/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.257729 | Val Loss = 0.304039\n",
            "  Time: 73.4s | LR: 5.00e-04\n",
            "  No improvement (8/10)\n",
            "\n",
            "[Epoch 017/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.250984 | Val Loss = 0.309288\n",
            "  Time: 73.1s | LR: 5.00e-04\n",
            "  No improvement (9/10)\n",
            "\n",
            "[Epoch 018/100]\n",
            "  Training: Batch 350/359 (97%)\n",
            "  Validation: Batch 44/45 (98%)\n",
            "  Results: Train Loss = 0.244215 | Val Loss = 0.308948\n",
            "  Time: 73.1s | LR: 5.00e-04\n",
            "  No improvement (10/10)\n",
            "\n",
            "  ‚ö† Early stopping triggered after 18 epochs (22.3 minutes)\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETE!\n",
            "  Total time: 22.3 minutes\n",
            "  Best validation loss: 0.287257\n",
            "  Final epoch: 18\n",
            "================================================================================\n",
            "\n",
            "8. Plotting training curves...\n",
            "  ‚úì Saved: /content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder/training_curves.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference and Comparison using Trained Model"
      ],
      "metadata": {
        "id": "B1yFuSxelnNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import torch.fft as fft\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, Dict, Any\n",
        "import numpy as np\n",
        "from data_store_load import load_full_dataset\n",
        "from fno_implementation_modified import (\n",
        "    compute_norm_stats, FNO2DSpacetime, OneStepDataset,\n",
        "    collate_batch, train_one_epoch, eval_one_epoch, autoregressive_rollout\n",
        ")"
      ],
      "metadata": {
        "id": "PfWrCHPv6wAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- imports you rely on (make sure these exist up top) ---\n",
        "import os, math, torch, torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# =========================\n",
        "# Root dir & path helpers\n",
        "# =========================\n",
        "try:\n",
        "    DIR = dir_path\n",
        "except Exception:\n",
        "    DIR = \"\"\n",
        "\n",
        "def P(fn: str) -> str:\n",
        "    \"Join to project directory for outputs/artifacts.\"\n",
        "    return os.path.join(DIR, fn)\n",
        "\n",
        "def find_input(filename: str) -> str:\n",
        "    cands = [\n",
        "        os.path.join(DIR, filename),\n",
        "        os.path.join('/mnt/data', filename),\n",
        "        filename,  # absolute / custom\n",
        "    ]\n",
        "    for p in cands:\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    raise FileNotFoundError(f\"Could not find input file '{filename}' in {cands}\")\n",
        "\n",
        "# Checkpoint path\n",
        "ckpt_path = P(\"fno_autoregressive.pt\")"
      ],
      "metadata": {
        "id": "vznHYxIp7JqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"=\"*80)\n",
        "    print(\"FNO 1D SPACE PIPELINE TRANSIENT MODEL INFERENCE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "   # ---- Configuration ----\n",
        "    IN_CHANNELS = 10   # All 10 variables as input\n",
        "    OUT_CHANNELS = 7   # First 7 variables as output\n",
        "\n",
        "    # ---- paths ----\n",
        "    dataset_file = find_input(\"cleaned_tensor.pt\")\n",
        "    print(f\"\\n1. Configuration\")\n",
        "    print(f\"   Dataset file: {dataset_file}\")\n",
        "    print(f\"   Input channels: {IN_CHANNELS}\")\n",
        "    print(f\"   Output channels: {OUT_CHANNELS}\")\n",
        "\n",
        "    # ---- check if dataset exists ----\n",
        "    if not os.path.exists(dataset_file):\n",
        "        print(f\"\\n‚ùå Error: Dataset file not found: {dataset_file}\")\n",
        "        print(\"   Please run data_store_load.py first to create the dataset.\")\n",
        "        return # Return None if dataset not found\n",
        "\n",
        "     # ---- load dataset ----\n",
        "    print(f\"\\n2. Loading dataset...\")\n",
        "    try:\n",
        "        # Load your tensor format\n",
        "        data = torch.load(dataset_file)\n",
        "\n",
        "        # Handle different possible formats\n",
        "        if isinstance(data, dict):\n",
        "            full_tensor = data['full_tensor']  # [50, 1279, 10, 288]\n",
        "            variable_names = data.get('variable_names', [f'Var{i}' for i in range(IN_CHANNELS)])\n",
        "        else:\n",
        "            full_tensor = data\n",
        "            variable_names = [f'Var{i}' for i in range(IN_CHANNELS)]\n",
        "\n",
        "        N, X, C, T = full_tensor.shape\n",
        "\n",
        "        print(f\"   ‚úì Dataset shape: {full_tensor.shape}\")\n",
        "        print(f\"   ‚úì Cases: {N}\")\n",
        "        print(f\"   ‚úì Sections: {X}\")\n",
        "        print(f\"   ‚úì Variables: {C}\")\n",
        "        print(f\"   ‚úì Timesteps: {T}\")\n",
        "        print(f\"   ‚úì Variable names: {variable_names}\")\n",
        "\n",
        "        # Validate dimensions\n",
        "        assert C >= IN_CHANNELS, f\"Expected at least {IN_CHANNELS} channels, got {C}\"\n",
        "        assert C >= OUT_CHANNELS, f\"Expected at least {OUT_CHANNELS} channels, got {C}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading dataset: {e}\")\n",
        "        return # Return None if error loading dataset\n",
        "\n",
        "    # Convert section_lengths to numpy if it's a tensor\n",
        "    #if isinstance(section_lengths, torch.Tensor):\n",
        "    #    section_lengths = section_lengths.numpy()\n",
        "\n",
        "    # Create section lengths (uniform or load actual if available)\n",
        "    section_lengths = np.arange(X, dtype=np.float32)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"   ‚úì Using device: {device}\")\n",
        "\n",
        "    # ---- splits (by case) ----\n",
        "    print(f\"\\n3. Creating train/val/test splits...\")\n",
        "    N = full_tensor.shape[0]\n",
        "    n_train = int(0.8 * N)\n",
        "    n_val   = int(0.1 * N)\n",
        "    n_test  = N - n_train - n_val\n",
        "\n",
        "    print(f\"   Total cases: {N}\")\n",
        "    print(f\"   Train: {n_train} cases ({100*n_train/N:.0f}%)\")\n",
        "    print(f\"   Val: {n_val} cases ({100*n_val/N:.0f}%)\")\n",
        "    print(f\"   Test: {n_test} cases ({100*n_test/N:.0f}%)\")\n",
        "\n",
        "    # simple contiguous split; switch to random if you prefer\n",
        "    train_tensor = full_tensor[:n_train]\n",
        "    val_tensor   = full_tensor[n_train:n_train+n_val]\n",
        "    test_tensor  = full_tensor[n_train+n_val:]\n",
        "\n",
        "     # in_ch = IN_CHANNELS + 3 (t_coord, x_coord, t0_mask)\n",
        "    model_in_ch = IN_CHANNELS + 3\n",
        "\n",
        "    # Normalization\n",
        "    mean_input, std_input, mean_output, std_output = compute_norm_stats(\n",
        "        train_tensor, IN_CHANNELS, OUT_CHANNELS\n",
        "    )\n",
        "    # Move normalization stats to the correct device\n",
        "    mean_input = mean_input.to(device)\n",
        "    std_input = std_input.to(device)\n",
        "    mean_output = mean_output.to(device)\n",
        "    std_output = std_output.to(device)\n",
        "\n",
        "    model = FNO2DSpacetime(\n",
        "        in_ch=model_in_ch,\n",
        "        out_ch=OUT_CHANNELS,\n",
        "        width=256, #Changed from 128 -> 256\n",
        "        depth=5,\n",
        "        k_t=1,   # KEY: Single mode in time (essentially spatial-only)\n",
        "        k_x=128,  # Spatial modes : Changed from 64 -> 128\n",
        "        predict_delta_from_u0=False\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "    # ---- Autoregressive inference on test case ----\n",
        "    print(f\"\\n Testing autoregressive rollout...\")\n",
        "    if len(test_tensor) > 0:\n",
        "      # Load best model\n",
        "      ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
        "      model.load_state_dict(ckpt[\"model\"])\n",
        "      model.eval()\n",
        "\n",
        "      # Test on first test case\n",
        "      test_case = test_tensor[0]  # [X, C, T]\n",
        "      initial_state = test_case[:, :, 0]  # [X, C]\n",
        "      ground_truth = test_case  # [X, C, T]\n",
        "\n",
        "      #n_rollout_steps = min(100, T-1)  # Rollout for 100 steps or less\n",
        "      n_rollout_steps = T-1\n",
        "\n",
        "      print(f\"  Rolling out {n_rollout_steps} timesteps...\")\n",
        "      predictions = autoregressive_rollout(\n",
        "          model, initial_state, ground_truth, section_lengths,\n",
        "          mean_input, std_input, mean_output, std_output,\n",
        "          IN_CHANNELS, OUT_CHANNELS, n_rollout_steps, device=device\n",
        "      )\n",
        "\n",
        "      # Calculate errors\n",
        "      gt = ground_truth[:, :OUT_CHANNELS, 1:n_rollout_steps+1].permute(1, 2, 0)  # [C_out, n_steps, X]\n",
        "      mse = ((predictions - gt) ** 2).mean()\n",
        "      mae = (predictions - gt).abs().mean()\n",
        "\n",
        "      print(f\"\\n11. Plotting SPATIAL profiles at specific timesteps...\")\n",
        "\n",
        "      # --- Define timesteps of interest ---\n",
        "      # (t=0 is the first prediction, t=99 is the last)\n",
        "      time_indices_to_plot = [0, 29, 99]\n",
        "      spatial_axis = np.arange(X) # X = 1279 sections\n",
        "\n",
        "      for t_idx in time_indices_to_plot:\n",
        "          # One figure per timestep, showing all 7 variables\n",
        "          fig, axes = plt.subplots(OUT_CHANNELS, 1, figsize=(14, 16), sharex=True)\n",
        "          fig.suptitle(f'Spatial Profile Comparison at Timestep t={t_idx+1}', fontsize=16)\n",
        "\n",
        "          for var_idx in range(OUT_CHANNELS):\n",
        "              ax = axes[var_idx]\n",
        "\n",
        "              # Get the spatial slice for this variable at this time\n",
        "              # Shape: [X]\n",
        "              pred_spatial = predictions[var_idx, t_idx, :].cpu().numpy()\n",
        "              gt_spatial   = gt[var_idx, t_idx, :].cpu().numpy()\n",
        "\n",
        "              ax.plot(spatial_axis, gt_spatial, label='Ground Truth', linewidth=1.5, alpha=0.8)\n",
        "              ax.plot(spatial_axis, pred_spatial, label='Prediction', linestyle='--', linewidth=1.5, alpha=0.8)\n",
        "\n",
        "              var_name = variable_names[var_idx] if var_idx < len(variable_names) else f'Var{var_idx}'\n",
        "              ax.set_title(var_name, fontweight='bold')\n",
        "              ax.set_ylabel('Value')\n",
        "              ax.legend(loc='best')\n",
        "              ax.grid(True, alpha=0.3)\n",
        "\n",
        "          axes[-1].set_xlabel('Spatial Section (X)')\n",
        "          plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "          spatial_plot_path = P(f'spatial_profile_t{t_idx+1}.png')\n",
        "          plt.savefig(spatial_plot_path, dpi=150)\n",
        "          print(f\"  ‚úì Saved spatial plot: {spatial_plot_path}\")\n",
        "          plt.close(fig)\n",
        "\n",
        "      print(f\"  ‚úì Rollout complete\")\n",
        "      print(f\"  Test MSE: {mse:.6f}\")\n",
        "      print(f\"  Test MAE: {mae:.6f}\")\n",
        "\n",
        "      # Plot predictions for middle section\n",
        "      print(f\"\\n10. Plotting predictions...\")\n",
        "      section_idx = X // 2\n",
        "\n",
        "      fig, axes = plt.subplots(4, 2, figsize=(14, 14))\n",
        "      fig.suptitle(f'Autoregressive Predictions vs Ground Truth\\nSection {section_idx}', fontsize=14)\n",
        "\n",
        "      for var_idx in range(min(7, OUT_CHANNELS)):\n",
        "          row = var_idx // 2\n",
        "          col = var_idx % 2\n",
        "\n",
        "          if var_idx == 6:\n",
        "              row, col = 3, 0\n",
        "\n",
        "          ax = axes[row, col]\n",
        "\n",
        "          pred = predictions[var_idx, :, section_idx].cpu().numpy()\n",
        "          truth = gt[var_idx, :, section_idx].cpu().numpy()\n",
        "          timesteps = np.arange(1, len(pred)+1)\n",
        "\n",
        "          ax.plot(timesteps, truth, label='Ground Truth', linewidth=1.5, alpha=0.7)\n",
        "          ax.plot(timesteps, pred, label='Prediction', linestyle='--', linewidth=1.5, alpha=0.7)\n",
        "\n",
        "          var_name = variable_names[var_idx] if var_idx < len(variable_names) else f'Var{var_idx}'\n",
        "          ax.set_title(var_name, fontweight='bold')\n",
        "          ax.set_xlabel('Timestep')\n",
        "          ax.set_ylabel('Value')\n",
        "          ax.legend(loc='best')\n",
        "          ax.grid(True, alpha=0.3)\n",
        "\n",
        "      # Hide empty subplot\n",
        "      axes[3, 1].axis('off')\n",
        "\n",
        "      plt.tight_layout()\n",
        "      pred_path = P('autoregressive_predictions.png')\n",
        "      plt.savefig(pred_path, dpi=150)\n",
        "      print(f\"  ‚úì Saved: {pred_path}\")\n",
        "      plt.close()\n",
        "\n",
        "      # # ... (after your plotting code) ...\n",
        "\n",
        "      # # ==========================================================\n",
        "      # #    NEW: EXPORT COMPARISON FOR SECTIONS 1 AND 1279\n",
        "      # # ==========================================================\n",
        "      # print(f\"\\n12. Exporting comparison data for all 10 variables...\")\n",
        "\n",
        "      # # --- Parameters ---\n",
        "      # sections_to_export = [0, 1278] # Section 1 (index 0) and 1279 (index 1278)\n",
        "      # case_to_export = 1\n",
        "\n",
        "      # # Get the variable names for all 10 columns\n",
        "      # all_var_names = [variable_names[i] for i in range(IN_CHANNELS)]\n",
        "\n",
        "      # # Define the output path\n",
        "      # excel_path = P(f\"Case{case_to_export}_edge_sections_ALL_VARS.xlsx\")\n",
        "\n",
        "      # # Use ExcelWriter to save multiple sheets in one file\n",
        "      # with pd.ExcelWriter(excel_path) as writer:\n",
        "      #     print(f\"  Saving to: {excel_path}\")\n",
        "\n",
        "      #     for section_idx in sections_to_export:\n",
        "      #         section_num = section_idx + 1\n",
        "      #         print(f\"    - Processing Section {section_num}...\")\n",
        "\n",
        "      #         # --- 1. Slice all Tensors ---\n",
        "\n",
        "      #         # A. Get the 7 PREDICTED variables (shape [100, 7])\n",
        "      #         pred_slice_7 = predictions[:, :, section_idx].permute(1, 0).cpu().numpy()\n",
        "\n",
        "      #         # B. Get the 7 GROUND TRUTH variables (shape [100, 7])\n",
        "      #         gt_slice_7   = gt[:, :, section_idx].permute(1, 0).cpu().numpy()\n",
        "\n",
        "      #         # C. Get the 3 \"CHEATER\" variables from the *original* ground_truth tensor\n",
        "      #         #    Shape is [X, C, T] -> [1279, 10, 288]\n",
        "      #         #    We slice it: [section, channels 7-9, timesteps 1-100]\n",
        "      #         cheater_slice = ground_truth[section_idx, OUT_CHANNELS:IN_CHANNELS, 1:n_rollout_steps+1]\n",
        "      #         # -> Shape is [3, 100], permute to [100, 3]\n",
        "      #         cheater_slice_np = cheater_slice.permute(1, 0).cpu().numpy()\n",
        "\n",
        "      #         # --- 2. Combine and Create Pandas DataFrames ---\n",
        "\n",
        "      #         # Combine 7 predicted + 3 cheater\n",
        "      #         pred_combined_10 = np.concatenate([pred_slice_7, cheater_slice_np], axis=1)\n",
        "      #         df_pred = pd.DataFrame(pred_combined_10, columns=all_var_names)\n",
        "      #         df_pred.insert(0, 'Timestep', np.arange(1, n_rollout_steps + 1))\n",
        "      #         df_pred.insert(0, 'DataType', 'Prediction')\n",
        "\n",
        "      #         # Combine 7 ground truth + 3 cheater\n",
        "      #         gt_combined_10 = np.concatenate([gt_slice_7, cheater_slice_np], axis=1)\n",
        "      #         df_gt = pd.DataFrame(gt_combined_10, columns=all_var_names)\n",
        "      #         df_gt.insert(0, 'Timestep', np.arange(1, n_rollout_steps + 1))\n",
        "      #         df_gt.insert(0, 'DataType', 'GroundTruth')\n",
        "\n",
        "      #         # --- 3. Combine and Save to Sheet ---\n",
        "      #         df_combined = pd.concat([df_gt, df_pred])\n",
        "\n",
        "      #         # Save to a new sheet in the same Excel file\n",
        "      #         df_combined.to_excel(writer, index=False, sheet_name=f'Section_{section_num}')\n",
        "\n",
        "      # print(f\"  ‚úì Successfully saved all sections.\")\n",
        "\n",
        "      print(f\"\\n\" + \"=\" * 80)\n",
        "      print(\"ALL DONE! üéâ\")\n",
        "      print(\"=\" * 80)\n",
        "      print(f\"\\nFiles saved:\")\n",
        "      print(f\"  - {ckpt_path}\")\n",
        "      print(f\"  - {P('training_curves.png')}\")\n",
        "      print(f\"  - {P('autoregressive_predictions.png')}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Call main\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GLtBDlz7ObN",
        "outputId": "3bd5720e-7749-4f82-902a-f0621deab4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "FNO 1D SPACE PIPELINE TRANSIENT MODEL INFERENCE\n",
            "================================================================================\n",
            "\n",
            "1. Configuration\n",
            "   Dataset file: /content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder/cleaned_tensor.pt\n",
            "   Input channels: 10\n",
            "   Output channels: 7\n",
            "\n",
            "2. Loading dataset...\n",
            "   ‚úì Dataset shape: torch.Size([50, 1279, 10, 288])\n",
            "   ‚úì Cases: 50\n",
            "   ‚úì Sections: 1279\n",
            "   ‚úì Variables: 10\n",
            "   ‚úì Timesteps: 288\n",
            "   ‚úì Variable names: ['PT', 'TM', 'HOL', 'HOLWT', 'UG', 'UL', 'ULWT', 'TT', 'PI', 'FI']\n",
            "   ‚úì Using device: cuda\n",
            "\n",
            "3. Creating train/val/test splits...\n",
            "   Total cases: 50\n",
            "   Train: 40 cases (80%)\n",
            "   Val: 5 cases (10%)\n",
            "   Test: 5 cases (10%)\n",
            "\n",
            " Testing autoregressive rollout...\n",
            "  Rolling out 287 timesteps...\n",
            "\n",
            "11. Plotting SPATIAL profiles at specific timesteps...\n",
            "  ‚úì Saved spatial plot: /content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder/spatial_profile_t1.png\n",
            "  ‚úì Saved spatial plot: /content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder/spatial_profile_t30.png\n",
            "  ‚úì Saved spatial plot: /content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder/spatial_profile_t100.png\n",
            "  ‚úì Rollout complete\n",
            "  Test MSE: 1.201953\n",
            "  Test MAE: 0.493004\n",
            "\n",
            "10. Plotting predictions...\n",
            "  ‚úì Saved: /content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder/autoregressive_predictions.png\n",
            "\n",
            "================================================================================\n",
            "ALL DONE! üéâ\n",
            "================================================================================\n",
            "\n",
            "Files saved:\n",
            "  - /content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder/fno_autoregressive.pt\n",
            "  - /content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder/training_curves.png\n",
            "  - /content/gdrive/MyDrive/Colab Notebooks/FourierNeuralOperator/Transient Folder/autoregressive_predictions.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SS11wO0AlmWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IGNORE\n"
      ],
      "metadata": {
        "id": "hG7KQmwupOb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fno_compare_inference.py\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# --- your modules ---\n",
        "from data_store_load import load_full_dataset\n",
        "from fno_implementation_modified import FNO2DSpacetime  # uses the safe-FFT spectral layer\n",
        "\n",
        "# --- imports you rely on (make sure these exist up top) ---\n",
        "import math\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# =========================\n",
        "# Root dir & path helpers\n",
        "# =========================\n",
        "try:\n",
        "    DIR = dir_path  # type: ignore[name-defined]\n",
        "except Exception:\n",
        "    DIR = \"\"\n",
        "\n",
        "def P(fn: str) -> str:\n",
        "    \"Join to project directory for outputs/artifacts.\"\n",
        "    return os.path.join(DIR, fn)\n",
        "\n",
        "def find_input(filename: str) -> str:\n",
        "    cands = [\n",
        "        os.path.join(DIR, filename),\n",
        "        os.path.join('/mnt/data', filename),\n",
        "        filename,  # absolute / custom\n",
        "    ]\n",
        "    for p in cands:\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    raise FileNotFoundError(f\"Could not find input file '{filename}' in {cands}\")\n",
        "\n",
        "# Ensure output dir exists (handles empty DIR too)\n",
        "os.makedirs(DIR or \".\", exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Utilities (reuse from train)\n",
        "# -------------------------\n",
        "def compute_norm_stats(train_tensor: torch.Tensor):\n",
        "    t = train_tensor.permute(0, 2, 1, 3).contiguous()  # [N,5,X,121]\n",
        "    mean = t.mean(dim=(0, 2, 3))                       # [5]\n",
        "    std  = t.std(dim=(0, 2, 3))                        # [5]\n",
        "    std = torch.where(std < 1e-6, torch.ones_like(std), std)\n",
        "    return mean, std\n",
        "\n",
        "@torch.no_grad()\n",
        "def make_tx_coords(T: int, X: int, section_lengths_m: np.ndarray, device=None):\n",
        "    t = torch.linspace(1.0, float(T), T, device=device) / float(T)  # [T]\n",
        "    sl = torch.tensor(section_lengths_m, dtype=torch.float32, device=device)  # [X]\n",
        "    x = (sl - sl.min()) / (sl.max() - sl.min() + 1e-12)\n",
        "    tt = t[:, None].expand(T, X)\n",
        "    xx = x[None, :].expand(T, X)\n",
        "    return tt[None, None], xx[None, None]  # [1,1,T,X] each\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_full_block(model: FNO2DSpacetime,\n",
        "                       case_tensor: torch.Tensor,          # [X,5,121]\n",
        "                       section_lengths_m: np.ndarray,\n",
        "                       mean: torch.Tensor, std: torch.Tensor,\n",
        "                       device: str = \"cpu\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Returns denormalized prediction [5, 120, X] for t=1..120\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    X = case_tensor.shape[0]\n",
        "    T = 120\n",
        "    # u0: [5,X]\n",
        "    u0 = case_tensor[:, :, 0].permute(1, 0).contiguous()\n",
        "    mean_c = mean.view(5, 1).to(device)\n",
        "    std_c  = std.view(5, 1).to(device)\n",
        "    u0n = (u0.to(device) - mean_c) / (std_c + 1e-8)  # [5,X]\n",
        "\n",
        "    # inputs\n",
        "    u0_tiled = u0n[:, None, :].expand(5, T, X).unsqueeze(0)     # [1,5,T,X]\n",
        "    t_coord, x_coord = make_tx_coords(T, X, section_lengths_m, device=device)\n",
        "    t0_mask = torch.zeros(1, 1, T, X, device=device); t0_mask[:, :, 0, :] = 1.0\n",
        "\n",
        "    # forward\n",
        "    yhat = model(u0_tiled, t_coord, x_coord, t0_mask).squeeze(0)  # [5,T,X]\n",
        "    # denormalize\n",
        "    yhat_denorm = yhat * std_c[:, None, :] + mean_c[:, None, :]\n",
        "    return yhat_denorm  # [5,120,X]\n",
        "\n",
        "def mae(a, b, dim=None):\n",
        "    return (a - b).abs().mean(dim=dim)\n",
        "\n",
        "def rmse(a, b, dim=None):\n",
        "    return torch.sqrt(((a - b) ** 2).mean(dim=dim))\n",
        "\n",
        "# -------------------------\n",
        "# Main comparison routine\n",
        "# -------------------------\n",
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"FNO TEST COMPARISON @ t=1h, 30h, 120h (3 cases, all 5 variables)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # ---- paths via find_input/P ----\n",
        "    dataset_file = find_input(\"full_dataset.pt\")\n",
        "    ckpt_path    = find_input(\"fno_full_block.pt\")  # load checkpoint from known locations\n",
        "\n",
        "    # ---- load dataset ----\n",
        "    full_tensor, section_lengths, time_hours, variable_names, case_numbers = load_full_dataset(dataset_file)\n",
        "    if isinstance(section_lengths, torch.Tensor):\n",
        "        section_lengths = section_lengths.numpy()\n",
        "\n",
        "    # ---- split the same way as training ----\n",
        "    N = full_tensor.shape[0]\n",
        "    n_train = int(0.8 * N)\n",
        "    n_val   = int(0.1 * N)\n",
        "    train_tensor = full_tensor[:n_train]\n",
        "    val_tensor   = full_tensor[n_train:n_train+n_val]\n",
        "    test_tensor  = full_tensor[n_train+n_val:]\n",
        "\n",
        "    # ---- normalization from train ----\n",
        "    mean, std = compute_norm_stats(train_tensor)\n",
        "\n",
        "    # ---- load model ----\n",
        "    model = FNO2DSpacetime(in_ch=8, out_ch=5, width=128, depth=5, k_t=8, k_x=32,\n",
        "                           predict_delta_from_u0=False).to(device)\n",
        "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    model.eval()\n",
        "\n",
        "    # ---- pick 3 test cases ----\n",
        "    num_cases = min(3, test_tensor.shape[0])\n",
        "    chosen = list(range(num_cases))\n",
        "    print(f\"Comparing test cases indices (within test split): {chosen}\")\n",
        "\n",
        "    # ---- times of interest (1h, 30h, 120h) -> indices [0, 29, 119]\n",
        "    times = {1: 0, 30: 29, 120: 119}\n",
        "\n",
        "    # ---- gather results & errors ----\n",
        "    error_rows = []\n",
        "    for rel_idx, case_idx in enumerate(chosen):\n",
        "        case = test_tensor[case_idx]  # [X,5,121]\n",
        "        pred = predict_full_block(model, case, section_lengths, mean, std, device=device)  # [5,120,X]\n",
        "        gt = case[:, :, 1:].permute(1, 2, 0).contiguous()  # [5,120,X]\n",
        "\n",
        "        X = case.shape[0]\n",
        "        x_axis = np.arange(X)\n",
        "\n",
        "        for t_hr, t_idx in times.items():\n",
        "            # One figure per case/time showing 5 variables as 5 subplots\n",
        "            fig, axes = plt.subplots(5, 1, figsize=(12, 14), sharex=True)\n",
        "            fig.suptitle(f\"Case {case_idx} (test idx {rel_idx}) ‚Äî Comparison at t = {t_hr} h\")\n",
        "\n",
        "            for v in range(5):\n",
        "                y_true = gt[v, t_idx].cpu()\n",
        "                y_pred = pred[v, t_idx].cpu()\n",
        "\n",
        "                ax = axes[v]\n",
        "                ax.plot(x_axis, y_true, label=\"Ground truth\")\n",
        "                ax.plot(x_axis, y_pred, label=\"Prediction\", linestyle=\"--\")\n",
        "                ax.set_ylabel(variable_names[v] if v < len(variable_names) else f\"Var{v}\")\n",
        "                ax.grid(True)\n",
        "                if v == 0:\n",
        "                    ax.legend()\n",
        "\n",
        "                # errors across X\n",
        "                v_mae = float(mae(y_pred, y_true, dim=0))\n",
        "                v_rmse = float(rmse(y_pred, y_true, dim=0))\n",
        "                error_rows.append({\n",
        "                    \"case\": case_idx,\n",
        "                    \"time_h\": t_hr,\n",
        "                    \"variable\": variable_names[v] if v < len(variable_names) else f\"Var{v}\",\n",
        "                    \"MAE\": v_mae,\n",
        "                    \"RMSE\": v_rmse\n",
        "                })\n",
        "\n",
        "            axes[-1].set_xlabel(\"Spatial index (X)\")\n",
        "            plt.tight_layout()\n",
        "            out_png = P(f\"compare_case{case_idx}_t{t_hr}.png\")\n",
        "            plt.savefig(out_png, dpi=140)\n",
        "            print(f\"Saved plot: {out_png}\")\n",
        "            plt.close(fig)\n",
        "\n",
        "    # ---- summary error table ----\n",
        "    df = pd.DataFrame(error_rows).sort_values([\"case\", \"time_h\", \"variable\"])\n",
        "    pd.set_option(\"display.max_rows\", None)\n",
        "    print(\"\\nPer-case, per-time, per-variable errors (MAE, RMSE):\")\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "    # Also print grouped means for a quick summary\n",
        "    print(\"\\nAverage errors grouped by time (across 3 cases):\")\n",
        "    print(df.groupby([\"time_h\", \"variable\"])[[\"MAE\", \"RMSE\"]].mean().round(4))\n",
        "\n",
        "    # Save CSV via P()\n",
        "    csv_path = P(\"comparison_errors.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nSaved CSV with all errors -> {csv_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "l_ZDm2-8-wxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IGNORE BELOW"
      ],
      "metadata": {
        "id": "YQCv2iaN52q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tm_outlier_scan.py (or run in a cell)\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- project helpers (same as your other scripts) ---\n",
        "try:\n",
        "    DIR = dir_path  # type: ignore[name-defined]\n",
        "except Exception:\n",
        "    DIR = \"\"\n",
        "\n",
        "def P(fn: str) -> str:\n",
        "    return os.path.join(DIR, fn)\n",
        "\n",
        "def find_input(filename: str) -> str:\n",
        "    cands = [os.path.join(DIR, filename), os.path.join('/mnt/data', filename), filename]\n",
        "    for p in cands:\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    raise FileNotFoundError(f\"Could not find input file '{filename}' in {cands}\")\n",
        "\n",
        "# --- config ---\n",
        "TM_UPPER_OK = 90.0   # expected max (change if needed)\n",
        "TOP_K = 20           # how many top offending cases to print\n",
        "\n",
        "# --- data load ---\n",
        "from data_store_load import load_full_dataset\n",
        "dataset_file = find_input(\"full_dataset.pt\")\n",
        "full_tensor, section_lengths, time_hours, variable_names, case_numbers = load_full_dataset(dataset_file)\n",
        "\n",
        "# figure out which channel is TM robustly\n",
        "try:\n",
        "    tm_idx = variable_names.index(\"TM\")\n",
        "except Exception:\n",
        "    tm_idx = 1  # fallback based on your order\n",
        "\n",
        "# shapes\n",
        "N, X, C, T = full_tensor.shape  # e.g., [200, 1278, 5, 121]\n",
        "\n",
        "# pull TM only: [N, X, T]\n",
        "tm = full_tensor[:, :, tm_idx, :]\n",
        "\n",
        "# per-case max & argmax (where it happens)\n",
        "flat = tm.reshape(N, -1)                       # [N, X*T]\n",
        "max_vals, flat_idx = torch.max(flat, dim=1)    # [N], [N]\n",
        "max_vals = max_vals.to(torch.float64)\n",
        "\n",
        "idx_x = (flat_idx % X).to(torch.int64)         # [N]\n",
        "idx_t = (flat_idx // X).to(torch.int64)        # [N]\n",
        "\n",
        "# fraction of points > TM_UPPER_OK per case\n",
        "over_mask = (tm > TM_UPPER_OK)\n",
        "over_counts = over_mask.sum(dim=(1, 2)).to(torch.int64)  # [N]\n",
        "total_points = torch.tensor(X * T, dtype=torch.int64)\n",
        "over_frac = (over_counts.to(torch.float64) / float(X * T))  # [N]\n",
        "\n",
        "# assemble dataframe\n",
        "df = pd.DataFrame({\n",
        "    \"case_idx\": np.arange(N, dtype=int),\n",
        "    \"case_number\": np.array(case_numbers, dtype=int) if isinstance(case_numbers, (list, tuple)) else np.arange(N, dtype=int),\n",
        "    \"TM_max\": max_vals.cpu().numpy(),\n",
        "    \"x_at_max\": idx_x.cpu().numpy(),\n",
        "    \"t_idx_at_max\": idx_t.cpu().numpy(),     # 0 = 1h, 29 = 30h, 120 = 121h if your t=0..120\n",
        "    \"frac_over_threshold\": over_frac.cpu().numpy(),\n",
        "    \"count_over_threshold\": over_counts.cpu().numpy(),\n",
        "})\n",
        "\n",
        "# filter offending cases\n",
        "bad = df[df[\"TM_max\"] > TM_UPPER_OK].sort_values([\"TM_max\", \"frac_over_threshold\"], ascending=False)\n",
        "\n",
        "# save & print\n",
        "out_csv = P(\"tm_outliers.csv\")\n",
        "os.makedirs(DIR or \".\", exist_ok=True)\n",
        "bad.to_csv(out_csv, index=False)\n",
        "\n",
        "print(f\"\\nTM upper OK threshold = {TM_UPPER_OK}\")\n",
        "print(f\"Total cases: {N} | Cases with TM_max > {TM_UPPER_OK}: {len(bad)}\")\n",
        "print(\"\\nTop offending cases:\")\n",
        "print(bad.head(TOP_K).to_string(index=False))\n",
        "\n",
        "print(f\"\\nFull list saved to: {out_csv}\")\n",
        "print(\"\\nNote: t_idx_at_max is zero-based for the prediction window; if your t=0 is the initial state,\")\n",
        "print(\"      then t_idx_at_max=0 corresponds to 1h, 29 -> 30h, 119 -> 120h.\")\n"
      ],
      "metadata": {
        "id": "od0LmPqu3NYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EQXXDCeQ44ah"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}